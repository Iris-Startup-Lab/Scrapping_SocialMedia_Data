{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38265c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import time\n",
    "from urllib.parse import quote\n",
    "from bs4 import BeautifulSoup\n",
    "from pyppeteer import launch\n",
    "from pyppeteer.errors import PageError\n",
    "#import gradio as gr\n",
    "#from langdetect import detect\n",
    "#from google_trans_new import google_translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nasync def test_detect_and_translate(text):\\n    translated = await detect_and_translate(text)\\n    print(f\"Texto original: {text}\")\\n    print(f\"Texto traducido: {translated}\")\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def scrape_post(url_or_shortcode: str) -> str:\n",
    "    \"\"\"Generate URL for fetching Instagram post data using GraphQL\"\"\"\n",
    "    if \"http\" in url_or_shortcode:\n",
    "        shortcode = url_or_shortcode.split(\"/p/\")[-1].split(\"/\")[0]\n",
    "    else:\n",
    "        shortcode = url_or_shortcode\n",
    "    variables = {\n",
    "        \"shortcode\": shortcode,\n",
    "        \"child_comment_count\": 10000,\n",
    "        \"fetch_comment_count\": 50,  # Adjust based on how many comments you want per request\n",
    "        \"parent_comment_count\": 10000,\n",
    "        \"has_threaded_comments\": True,\n",
    "    }\n",
    "    query_hash = \"b3055c01b4b222b8a47dc12b090e4e64\"\n",
    "    doc_id = \"18113378221181848\"\n",
    "    encoded_variables = quote(json.dumps(variables))\n",
    "    url = f\"https://www.instagram.com/graphql/query/?query_hash={query_hash}&variables={encoded_variables}\"\n",
    "    url = f\"https://www.instagram.com/graphql/query/?doc_id={doc_id}&variables={encoded_variables}\"\n",
    "    \n",
    "    return url\n",
    "\n",
    "async def load_page(url):\n",
    "    browser = await launch(options={'args': ['--no-sandbox', '--disable-setuid-sandbox']})\n",
    "    page = await browser.newPage()\n",
    "    await page.goto(url)\n",
    "    await asyncio.sleep(3)  # Adjust as needed for page loading time\n",
    "    content = await page.content()\n",
    "    await browser.close()\n",
    "    return content\n",
    "\n",
    "\n",
    "def save_in_file(data, file_name):\n",
    "    \"\"\"Save data to a JSON file\"\"\"\n",
    "    with open(file_name, \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(data, file, indent=2, ensure_ascii=False)\n",
    "\n",
    "def extract_comments(parsed_data):\n",
    "    \"\"\"Extract comments from parsed JSON data\"\"\"\n",
    "    comments = []\n",
    "    try:\n",
    "        edges = parsed_data['data']['shortcode_media']['edge_media_to_parent_comment']['edges']\n",
    "        for edge in edges:\n",
    "            comment = edge['node']['text'].strip()\n",
    "            comments.append(comment)\n",
    "            if 'edge_threaded_comments' in edge['node']:\n",
    "                replies = edge['node']['edge_threaded_comments']['edges']\n",
    "                for reply in replies:\n",
    "                    reply_comment = reply['node']['text'].strip()\n",
    "                    comments.append(reply_comment)\n",
    "    except KeyError:\n",
    "        print(\"Error: Unable to extract comments. Check if the data structure has changed.\")\n",
    "    return comments\n",
    "\n",
    "async def scrape_comments(url):\n",
    "    try:\n",
    "        post_name = url.split('/')[-2]  # Extract post name from URL\n",
    "\n",
    "        # Load all comments dynamically\n",
    "        graphql_url = scrape_post(url)\n",
    "        content = await load_page(graphql_url)\n",
    "\n",
    "        # Parse content and extract comments\n",
    "        parsed_data = json.loads(BeautifulSoup(content, \"html.parser\").find(\"pre\").text)\n",
    "\n",
    "        if parsed_data:\n",
    "            comments = extract_comments(parsed_data)\n",
    "\n",
    "            # Translate comments\n",
    "            #translated_comments = await asyncio.gather(*[detect_and_translate(comment) for comment in comments])\n",
    "\n",
    "            # Save comments to JSON with post name as key\n",
    "            comments_data = {\n",
    "                post_name: comments\n",
    "            }\n",
    "            save_in_file(comments_data, f'{post_name}_comments.json')\n",
    "\n",
    "            return comments, f\"Comments saved successfully in {post_name}_comments.json.\"\n",
    "        else:\n",
    "            return [], \"Failed to load comments. Please check the URL or try again.\"\n",
    "    except PageError as pe:\n",
    "        return [], f\"Error: {pe}\"\n",
    "    except Exception as e:\n",
    "        return [], str(e)\n",
    "'''\n",
    "async def detect_and_translate(comment, target_lang=\"en\"):\n",
    "    \"\"\"Detect language of the comment and translate it to target language if needed\"\"\"\n",
    "    result_lang = detect(comment)\n",
    "\n",
    "    if result_lang == target_lang or result_lang == 'en':\n",
    "        return comment\n",
    "    else:\n",
    "        translator = google_translator()\n",
    "        translated_text = await translator.translate(comment, lang_src=result_lang, lang_tgt=target_lang)\n",
    "        return translated_text\n",
    "\n",
    "def gradio_interface(url):\n",
    "    translated_comments, message = asyncio.get_event_loop().run_until_complete(scrape_comments(url))\n",
    "    return \"\\n\".join(translated_comments), message\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=gradio_interface,\n",
    "    inputs=gr.Textbox(lines=2, placeholder=\"Enter Instagram post URL here...\"),\n",
    "    outputs=[gr.Textbox(lines=10, label=\"Translated Comments\"), gr.Textbox(label=\"Status Message\")],\n",
    "    title=\"Instagram Comment Scraper with Translation\",\n",
    "    description=\"Enter the URL of an Instagram post to scrape and translate comments.\",\n",
    ")\n",
    "\n",
    "# Para probar la interfaz de Gradio (esto bloqueará la ejecución en un entorno de Jupyter)\n",
    "# iface.launch(debug=True, share=True)\n",
    "'''\n",
    "\n",
    "# Funciones para pruebas individuales (ejecutar en celdas separadas)\n",
    "\n",
    "\n",
    "'''\n",
    "async def test_detect_and_translate(text):\n",
    "    translated = await detect_and_translate(text)\n",
    "    print(f\"Texto original: {text}\")\n",
    "    print(f\"Texto traducido: {translated}\")\n",
    "'''\n",
    "# Ejemplo de uso para pruebas (puedes descomentar y ejecutar en celdas separadas)\n",
    "# instagram_url = \"https://www.instagram.com/p/CwdT65hsj7G/\"\n",
    "# asyncio.run(test_scrape_comments(instagram_url))\n",
    "\n",
    "# Ejemplo de prueba de traducción\n",
    "# asyncio.run(test_detect_and_translate(\"Este es un comentario en español.\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c7ca9998",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Users\\1167486\\AppData\\Local\\anaconda3\\envs\\Iris_WebScrapper\\Lib\\site-packages\\pyppeteer\\util.py:29: RuntimeWarning: coroutine 'run_scrape_test' was never awaited\n",
      "  gc.collect()\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "[INFO] Starting Chromium download.\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Chromium downloadable not found at https://storage.googleapis.com/chromium-browser-snapshots/Win_x64/1181205/chrome-win.zip: Received <?xml version='1.0' encoding='UTF-8'?><Error><Code>NoSuchKey</Code><Message>The specified key does not exist.</Message><Details>No such object: chromium-browser-snapshots/Win_x64/1181205/chrome-win.zip</Details></Error>.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m load_page(scrape_post(instagram_url))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mload_page\u001b[39m\u001b[34m(url)\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_page\u001b[39m(url):\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m     browser = \u001b[38;5;28;01mawait\u001b[39;00m launch(options={\u001b[33m'\u001b[39m\u001b[33margs\u001b[39m\u001b[33m'\u001b[39m: [\u001b[33m'\u001b[39m\u001b[33m--no-sandbox\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m--disable-setuid-sandbox\u001b[39m\u001b[33m'\u001b[39m]})\n\u001b[32m     24\u001b[39m     page = \u001b[38;5;28;01mawait\u001b[39;00m browser.newPage()\n\u001b[32m     25\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m page.goto(url)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Users\\1167486\\AppData\\Local\\anaconda3\\envs\\Iris_WebScrapper\\Lib\\site-packages\\pyppeteer\\launcher.py:307\u001b[39m, in \u001b[36mlaunch\u001b[39m\u001b[34m(options, **kwargs)\u001b[39m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlaunch\u001b[39m(options: \u001b[38;5;28mdict\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m, **kwargs: Any) -> Browser:\n\u001b[32m    240\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Start chrome process and return :class:`~pyppeteer.browser.Browser`.\u001b[39;00m\n\u001b[32m    241\u001b[39m \u001b[33;03m    This function is a shortcut to :meth:`Launcher(options, **kwargs).launch`.\u001b[39;00m\n\u001b[32m    242\u001b[39m \u001b[33;03m    Available options are:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    305\u001b[39m \u001b[33;03m        option with extreme caution.\u001b[39;00m\n\u001b[32m    306\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m307\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[43mLauncher\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m.launch()\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Users\\1167486\\AppData\\Local\\anaconda3\\envs\\Iris_WebScrapper\\Lib\\site-packages\\pyppeteer\\launcher.py:120\u001b[39m, in \u001b[36mLauncher.__init__\u001b[39m\u001b[34m(self, options, **kwargs)\u001b[39m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.chromeExecutable:\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m check_chromium():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[43mdownload_chromium\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    121\u001b[39m     \u001b[38;5;28mself\u001b[39m.chromeExecutable = \u001b[38;5;28mstr\u001b[39m(chromium_executable())\n\u001b[32m    123\u001b[39m \u001b[38;5;28mself\u001b[39m.cmd = [\u001b[38;5;28mself\u001b[39m.chromeExecutable] + \u001b[38;5;28mself\u001b[39m.chromeArguments\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Users\\1167486\\AppData\\Local\\anaconda3\\envs\\Iris_WebScrapper\\Lib\\site-packages\\pyppeteer\\chromium_downloader.py:138\u001b[39m, in \u001b[36mdownload_chromium\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdownload_chromium\u001b[39m() -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    137\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Download and extract chromium.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m     extract_zip(\u001b[43mdownload_zip\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m, DOWNLOADS_FOLDER / REVISION)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Users\\1167486\\AppData\\Local\\anaconda3\\envs\\Iris_WebScrapper\\Lib\\site-packages\\pyppeteer\\chromium_downloader.py:82\u001b[39m, in \u001b[36mdownload_zip\u001b[39m\u001b[34m(url)\u001b[39m\n\u001b[32m     80\u001b[39m r = http.request(\u001b[33m'\u001b[39m\u001b[33mGET\u001b[39m\u001b[33m'\u001b[39m, url, preload_content=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m r.status >= \u001b[32m400\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mChromium downloadable not found at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[33m'\u001b[39m \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mReceived \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mr.data.decode()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m     84\u001b[39m \u001b[38;5;66;03m# 10 * 1024\u001b[39;00m\n\u001b[32m     85\u001b[39m _data = BytesIO()\n",
      "\u001b[31mOSError\u001b[39m: Chromium downloadable not found at https://storage.googleapis.com/chromium-browser-snapshots/Win_x64/1181205/chrome-win.zip: Received <?xml version='1.0' encoding='UTF-8'?><Error><Code>NoSuchKey</Code><Message>The specified key does not exist.</Message><Details>No such object: chromium-browser-snapshots/Win_x64/1181205/chrome-win.zip</Details></Error>.\n"
     ]
    }
   ],
   "source": [
    "await load_page(scrape_post(instagram_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15bff077",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def test_scrape_comments(url):\n",
    "    comments, message = await scrape_comments(url)\n",
    "    print(\"Mensaje:\", message)\n",
    "    if comments:\n",
    "        print(\"Primeros 10 comentarios traducidos:\")\n",
    "        for i, comment in enumerate(comments[:10]):\n",
    "            print(f\"{i+1}. {comment}\")\n",
    "    else:\n",
    "        print(\"No se encontraron comentarios o hubo un error.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define una URL de Instagram para probar\n",
    "instagram_url = \"https://www.instagram.com/katyperry/reel/DJfNSmdRaLj/\" # Reemplaza con una URL válida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prueba la función de scraping de comentarios\n",
    "async def run_scrape_test(url):\n",
    "    await test_scrape_comments(url)\n",
    "\n",
    "# Ejecuta la prueba (esto bloqueará la celda hasta que termine)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "245f6001",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_scrape_test(url):\n",
    "    await test_scrape_comments(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "935a7d16",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43masyncio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_scrape_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstagram_url\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Users\\1167486\\AppData\\Local\\anaconda3\\envs\\Iris_WebScrapper\\Lib\\asyncio\\runners.py:191\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(main, debug, loop_factory)\u001b[39m\n\u001b[32m    161\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[32m    162\u001b[39m \n\u001b[32m    163\u001b[39m \u001b[33;03mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    187\u001b[39m \u001b[33;03m    asyncio.run(main())\u001b[39;00m\n\u001b[32m    188\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m events._get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    190\u001b[39m     \u001b[38;5;66;03m# fail fast with short traceback\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    192\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    194\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Runner(debug=debug, loop_factory=loop_factory) \u001b[38;5;28;01mas\u001b[39;00m runner:\n\u001b[32m    195\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m runner.run(main)\n",
      "\u001b[31mRuntimeError\u001b[39m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "asyncio.run(run_scrape_test(instagram_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prueba la función de detección y traducción\n",
    "async def run_translate_test(text):\n",
    "    await test_detect_and_translate(text)\n",
    "\n",
    "# Ejecuta la prueba de traducción\n",
    "# asyncio.run(run_translate_test(\"Hola, ¿cómo estás?\"))\n",
    "# asyncio.run(run_translate_test(\"This is an English comment.\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Iris_WebScrapper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
